<!DOCTYPE HTML>
<!--
	Arcana by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>SPC Lab</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
	</head>
	<body>
		<div id="page-wrapper">

			<!-- Header -->
				<div id="header">

					<!-- Logo -->
						<h1><a href="index.html" id="logo">SPC LAB</em></a></h1>

					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li><a href="index.html">Home</a></li>
								<li><a href="research-lab.html">SPC Lab</a></li>
								<li class="current">
									<a href="#">Projects</a>
									<ul>
										<li>
											<a href="#">Current Projects</a>
											<ul>
												<li><a href="proj_compr.html">Compression</a></li>
												<li><a href="proj_soc_comp.html">Social Computing</a></li>
												<li><a href="proj_mol_comm.html">Molecular Communication</a></li>
												<li><a href="proj_mol_sensing.html">Biomarker Sensing</a></li>
												<li><a href="proj_bci.html">Neural Computing</a></li>
												<li><a href="proj_ac.html">Approximate Computing</a></li>
											</ul>
										</li>		
										<li>
											<a href="#">Past Projects</a>
											<ul>
												<li><a href="proj_CT.html">Coding Theory</a></li>												
												<li><a href="proj_SEC.html">Security</a></li>												
												<li><a href="proj_NIT.html">Network Information Theory</a></li>
											</ul>
										</li>		
									</ul>	
								</li>
								<li><a href="publications.html">Publications</a></li>
								<li><a href="people.html">People</a></li>
								<li>
									<a href="#">Courses</a>
									<ul>
										<li>
											<a href="#">Undergraduate</a>
											<ul>
												<li><a href="course_ece2026.html">ECE 2026</a></li>
												<li><a href="course_ece4270.html">ECE 4270</a></li>
												<li><a href="course_ece4601.html">ECE 4601</a></li>
											</ul>
										</li>
										<li>
											<a href="#">Graduate</a>
											<ul>
												<li><a href="course_ece6280.html">ECE 6280</a></li>
												<li><a href="course_ece6605.html">ECE 6605</a></li>
												<li><a href="course_ece6606.html">ECE 6606</a></li>
											</ul>
										</li>
									</ul>
								</li>
								<li><a href="personal.html">Personal</a></li>
								<li><a href="sponsors.html">Sponsors</a></li>
								<li><a href="https://www.ece.gatech.edu" target="_blank">ECE</a></li>
								<li><a href="contacts.html">Contacts</a></li>
						</nav>

				</div>
				<!--<section id="banner">-->
				<!--	<header>-->
				<!--		<h1>Compression</h1>-->
				<!--	</header>-->
				<!--</section>-->

			<!-- Main -->
				<section class="wrapper style1">
					<div class="container">
<!--						<header><center><h1><font size="10">Network Information Theory</font></h1></center></header> -->
						</br>
						<div class="row 200%">
							<div class="3u 12u(narrower)">
								<div id="sidebar">

									<!-- Sidebar -->
<!--
										<section>
											<h3>Just a Subheading</h3>
											<p>Phasellus quam turpis, feugiat sit amet ornare in, hendrerit in lectus.
											Praesent semper mod quis eget mi. Etiam eu ante risus. Aliquam erat volutpat.
											Aliquam luctus et mattis lectus sit amet pulvinar. Nam turpis et nisi etiam.</p>
											<footer>
												<a href="#" class="button">Continue Reading</a>
											</footer>
										</section>
-->
										<section class="wrapper style21">
<!--												
											<h3>Another Subheading</h3>
											<ul class="links">
												<li><a href="#">Amet turpis, feugiat et sit amet</a></li>
												<li><a href="#">Ornare in hendrerit in lectus</a></li>
												<li><a href="#">Semper mod quis eget mi dolore</a></li>
												<li><a href="#">Consequat etiam lorem phasellus</a></li>
												<li><a href="#">Amet turpis, feugiat et sit amet</a></li>
												<li><a href="#">Semper mod quisturpis nisi</a></li>
											</ul>
-->											
											<ul class="links">
												<li id="tabs1" onclick="showStuff(this)"><a style="cursor:pointer;">Network Compression</a></li>
												<li id="tabs2" onclick="showStuff(this)"><a style="cursor:pointer;">Seismic Compression</a></li>
												<li id="tabs3" onclick="showStuff(this)"><a style="cursor:pointer;">Model Compression</a></li>
												
											</ul>
<!--											
											<footer>
												<a href="#" class="button">More Random Links</a>
											</footer>
-->											
										</section>

								</div>
							</div>
							<div class="9u  12u(narrower) important(narrower)">
								<div id="content">

									<!-- Content -->

										<article>
<!--
											<header>
												<h2>Network Compression</h2>
											</header>
											<span class="image featured"><img src="images/banner.jpg" alt="" /></span>

											<p>Phasellus quam turpis, feugiat sit amet ornare in, hendrerit in lectus.
											Praesent semper mod quis eget mi. Etiam eu ante risus. Aliquam erat volutpat.
											Aliquam luctus et mattis lectus sit amet pulvinar. Nam turpis nisi
											consequat etiam lorem ipsum dolor sit amet nullam.</p>

											<h3>And Yet Another Subheading</h3>
											<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas ac quam risus, at tempus
											justo. Sed dictum rutrum massa eu volutpat. Quisque vitae hendrerit sem. Pellentesque lorem felis,
											ultricies a bibendum id, bibendum sit amet nisl. Mauris et lorem quam. Maecenas rutrum imperdiet
											vulputate. Nulla quis nibh ipsum, sed egestas justo. Morbi ut ante mattis orci convallis tempor.
											Etiam a lacus a lacus pharetra porttitor quis accumsan odio. Sed vel euismod nisi. Etiam convallis
											rhoncus dui quis euismod. Maecenas lorem tellus, congue et condimentum ac, ullamcorper non sapien.
											Donec sagittis massa et leo semper a scelerisque metus faucibus. Morbi congue mattis mi.
											Phasellus sed nisl vitae risus tristique volutpat. Cras rutrum commodo luctus.</p>

											<p>Phasellus odio risus, faucibus et viverra vitae, eleifend ac purus. Praesent mattis, enim
											quis hendrerit porttitor, sapien tortor viverra magna, sit amet rhoncus nisl lacus nec arcu.
											Suspendisse laoreet metus ut metus imperdiet interdum aliquam justo tincidunt. Mauris dolor urna,
											fringilla vel malesuada ac, dignissim eu mi. Praesent mollis massa ac nulla pretium pretium.
											Maecenas tortor mauris, consectetur pellentesque dapibus eget, tincidunt vitae arcu.
											Vestibulum purus augue, tincidunt sit amet iaculis id, porta eu purus.</p>
-->
											<section>
												<div id="tabs-1" class="tabContent">
													<header>
														<h2>Network Compression</h2>
													</header>
													<center>
													<span class="image featured_projects"><img src="images/projects/NetCompr.png" alt="" /></span>
													</center>
													<p>Several studies have shown the presence of significant redundancy in network traffic content. These
													studies clearly establish the tremendous scope for enhancing communication performance through
													exploitation of the redundancies. In this research, we introduce the concept of Network Compression
													via Network Memory. In the nutshell, network compression enables memorization of data traffic as it
													flows naturally (or by design) through the network. As such, memory enabled nodes can learn the
													source statistics which can then be used toward reducing the cost of describing the source in
													compression.</p>
													<p>We have proposed practical algorithms for network compression. We reported a memorization gain
													(defined as the additional redundancy elimination gain on top and over what can be achieved by the
													existing compression techniques) of order three when the algorithms are tested on the real Internet
													traffic traces. Furthermore, we investigated the deployment of memory in both random and Internetlike
													power-law graphs on real traffic. Our study showed significant network-wide gain of memorization
													even when a tiny fraction of the total number of nodes in the network is equipped with memory.</p>
													<center>
													<span class="image featured_projects"><img src="images/projects/NetCompr2.png" alt="" />Compression of Internet data via CTW scheme: without memory m=0 and with memory (i.e. m>0)</span>
													</center>
													<p>Network packet compression (at layer 3 or network layer) is a new paradigm with profound impacts in many applications ranging from efficient content delivery between data centers, in clouds and peer to peer networks to cellular, Wi-Fi and wireless sensor networks. We plan to tackle some of the requirements that will pave the way for the application of network packet compression via memory in these areas. In particular, we study several theoretical and practical research problems raised in packet compression, i.e., universal compression in finite-length regimes:
														<ul class="para_embed">
                                                            <li>Study of redundancy in universal compression with/without side information in finite lengths.</li>
															<li>Design and analysis of universal compression via memory for a compound/mixture source in finite lengths.</li>
															<li>Extension to multiple spatially-separated correlated sources.</li>
															<li>Develop memory-assisted network packet compression for non-stationary contents.</li>
															<li>Develop novel hardware acceleration for memory-assisted universal packet compression.</li>
															<li>Abundant applications of network compression can be identified in general wired and wireless networks, data centers, mobile-to-mobile computing, and sensor networks.</li>
                                                        </ul>
													
													
													</p>
												</div>
												<div id="tabs-2" class="tabContent">
													<header>
														<h2>Seismic Compression</h2>
													</header>
													<center>
													<span class="image featured"><img src="images/projects/seis1.png" alt="" /></span>
													<span class="image featured"><img src="images/projects/seis2.png" alt="" /></span>
													</center>
													<br>
													<p>
														The traditional seismic acquisition systems use passive recording devices and place all the processing at the data centers, hence require lots of data to be transferred from sensors to the recording systems. The trend for the future generations of seismic data acquisition is to have 1. Large scale seismic acquisition, which will produce a large amount of data to be collected daily, 2. Real time acquisition and quality assessment, and 3. Automation and adaptive seismic acquisition. Since cabling and labor costs constitute more than half of the costs of seismic acquisition, we propose to explore wireless seismic data acquisition. To meet the bandwidth and latency challenges, we propose to deviate from the traditional systems, by placing intelligence at the field by employing some processing power in the sensors.
													</p>
													<p>
														In this project, we investigate data compression techniques in the field for the transfer and storage of large scale seismic data via wireless networks. Specifically, we propose removing inter and intra trace redundancies (i.e., temporal and spatial correlations) in universal and distributed manner, without requiring any coordination among sensors. The premise of our proposed approach lies on learning from the seismic data on the go and utilizing this knowledge toward universal (near) lossless compression of the original traces. This approach is designed to address the key constraints of the network and recording system while maintaining the desirable objectives (e.g., locality of traces).
													</p>
													<p>
														In summary, there is a need for bringing new advances in seismic acquistion systems. Our group particularly studies the following critical topics:
														<ol class="para_embed">
                                                            <li>In-field processing where sensors become active and smart in the field</li>
															<ol class="para_embed_alpha">
																<li>Dictionary learning for seismic signal compression</li>
																<li>A framework to address the "large alphabet problem"</li>
																<li>A statistical lossless compression scheme using common memory</li>
																<li>Model forming between encoder and decoder (for Hardware-Aware statistical compression)</li>
																<li>Developing lossy compression engines based on Deep Neural Networks</li>
															</ol>
															<li>Wireless Autonomous sensors to acquire the data, and modify the topology</li>
                                                        </ol>
													<center>
													<span class="image featured"><img src="images/projects/seis3.png" alt="" /></span>
													</center>
													</p>
												</div>
												<div id="tabs-3" class="tabContent">
													<header>
														<h2>Model Complexity Reduction of Deep Neural Networks</h2>
													</header>
													<p>The sizes of neural networks have been growing rapidly as the complexity of the problems that they solve and the size of data keep increasing every year. They require more resources for the storage and computations. This problem compounds as deep neural networks become popular for solving challenging data analytic problems. Our objective is to substantially reduce the number of parameters required to represent a deep neural network without sacrificing predictive accuracy. For example, deep networks such as AlexNet has 61 million parameters, requiring 200 MB of memory. Clearly, such a memory requirement is very undesirable for many applications as it consumes a large amount of power due to off chip storage. Further, a deep network is composed of layers with very different properties. Convolutional layers, which contain a small fraction of the network parameters, yet require large computational effort. In contrast, fully connected layers contain the vast majority of the parameters and are dominated by data transfer. This imbalance between memory and computation suggests that we must address the efficiency of these two types of layers differently.</p>
													<p>We propose to leverage on neural network redundancies and develop a method to jointly quantize and compress large neural network models so that they can be fit in much smaller memory sizes (e.g., possibly in chip memory). This can significantly improve power consumption as well as latency of the computations over large neural networks.</p>
													<center>
													<span class="image featured"><img src="images/projects/model_compr.png" alt="" /></span>
													</center>														
												</div>												
											</section>
										</article>

								</div>
							</div>
						</div>
					</div>
				</section>

			<!-- Footer -->
				<div id="footer">
<!--					
					<div class="container">
						<div class="row">
							<section class="3u 6u(narrower) 12u$(mobilep)">
								<h3>Links to Stuff</h3>
								<ul class="links">
									<li><a href="#">Mattis et quis rutrum</a></li>
									<li><a href="#">Suspendisse amet varius</a></li>
									<li><a href="#">Sed et dapibus quis</a></li>
									<li><a href="#">Rutrum accumsan dolor</a></li>
									<li><a href="#">Mattis rutrum accumsan</a></li>
									<li><a href="#">Suspendisse varius nibh</a></li>
									<li><a href="#">Sed et dapibus mattis</a></li>
								</ul>
							</section>
							<section class="3u 6u$(narrower) 12u$(mobilep)">
								<h3>More Links to Stuff</h3>
								<ul class="links">
									<li><a href="#">Duis neque nisi dapibus</a></li>
									<li><a href="#">Sed et dapibus quis</a></li>
									<li><a href="#">Rutrum accumsan sed</a></li>
									<li><a href="#">Mattis et sed accumsan</a></li>
									<li><a href="#">Duis neque nisi sed</a></li>
									<li><a href="#">Sed et dapibus quis</a></li>
									<li><a href="#">Rutrum amet varius</a></li>
								</ul>
							</section>
							<section class="6u 12u(narrower)">
								<h3>Get In Touch</h3>
								<form>
									<div class="row 50%">
										<div class="6u 12u(mobilep)">
											<input type="text" name="name" id="name" placeholder="Name" />
										</div>
										<div class="6u 12u(mobilep)">
											<input type="email" name="email" id="email" placeholder="Email" />
										</div>
									</div>
									<div class="row 50%">
										<div class="12u">
											<textarea name="message" id="message" placeholder="Message" rows="5"></textarea>
										</div>
									</div>
									<div class="row 50%">
										<div class="12u">
											<ul class="actions">
												<li><input type="submit" class="button alt" value="Send Message" /></li>
											</ul>
										</div>
									</div>
								</form>
							</section>
						</div>
					</div>
-->
					<!-- Icons -->
<!--
						<ul class="icons">
							<li><a href="#" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="#" class="icon fa-facebook"><span class="label">Facebook</span></a></li>
							<li><a href="#" class="icon fa-github"><span class="label">GitHub</span></a></li>
							<li><a href="#" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
							<li><a href="#" class="icon fa-google-plus"><span class="label">Google+</span></a></li>
						</ul>
-->
					<!-- Copyright -->
						<div class="copyright">
							<ul class="menu">
								<li>&copy; SPC <script type="text/javascript">var year = new Date();document.write(year.getFullYear());</script>. All rights reserved</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>

				</div>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>
			<script type="text/javascript">
				// Content changer
				function showStuff(element)  {
					var tabContents = document.getElementsByClassName('tabContent');
					for (var i = 0; i < tabContents.length; i++) { 
						tabContents[i].style.display = 'none';
					}
				
					// change tabsX into tabs-X in order to find the correct tab content
					var tabContentIdToShow = element.id.replace(/(\d)/g, '-$1');
					document.getElementById(tabContentIdToShow).style.display = 'block';
				}
				
				// When the window is loaded intialize it with this
				function init() {
					document.getElementById('tabs-1').style.display = 'block';
                }
				window.onload = init()
			</script>
	</body>
</html>